# Required
PROJECT_ID=your-gcp-project-id
PROJECT_NUMBER=123456789012
REGION=us-central1
BQ_LOCATION=US

# Artifact Registry & image coordinates
# Use one repo for all images to match release workflow
AR_REPO=dbt-artifacts
IMAGE_NAME=dbt-runner
IMAGE_TAG=latest

# BigQuery datasets
# PROD_DATASET is required. INT_DATASET is optional; leave blank to skip creating a standing
# integration dataset (useful when you want a manual integration area separate from the ephemeral
# CI datasets created).
PROD_DATASET=analytics
INT_DATASET=

# Harden ACLs so only the respective service account can write, others query-only
# Set to false to skip hardening.
PROD_HARDEN_ACL=true
INT_HARDEN_ACL=true

# GCS buckets (no gs:// prefix)
# Bucket selection logic:
# - If only DBT_ARTIFACTS_BUCKET is set → use it for both artifacts and docs.
# - If only DBT_DOCS_BUCKET is set → deploy will pass it as artifacts bucket too.
# - If both are set → artifacts go to DBT_ARTIFACTS_BUCKET; docs go to DBT_DOCS_BUCKET.
DBT_DOCS_BUCKET=dbt-docs-bucket
DBT_ARTIFACTS_BUCKET=

# Optional: separate CI and PROD projects (defaults to PROJECT_ID if empty)
CI_PROJECT_ID=
PROD_PROJECT_ID=

# GitHub repo to trust via WIF (owner/repo)
GITHUB_REPO=your-org/your-repo
GITHUB_OWNER=your-org

# Optional: override WIF pool/provider ids used for secrets script
# POOL_ID=github-pool
# PROVIDER_ID=github-provider

# Service account IDs (left side of the @ address)
CI_SA_ID=dbt-ci
PROD_SA_ID=dbt-prod
SCHEDULER_SA_ID=dbt-scheduler
DOCS_VIEWER_SA_ID=dbt-docs-viewer

# Docs Viewer (Cloud Run service)
DOCS_VIEWER_SERVICE=dbt-docs-viewer

# Optional: who can view docs (grant run.invoker)
# Example: group:[email protected] or user:[email protected]
DOCS_VIEWER_ALLOWED_PRINCIPAL=

# Monitoring & alerting
# Comma-separated list of notification channel IDs (from Cloud Monitoring)
NOTIFICATION_CHANNELS=

# Docs generation in prod job (Cloud Run Job)
# If true, job runs `dbt docs generate --static` and uploads to DBT_DOCS_BUCKET (falls back to DBT_ARTIFACTS_BUCKET if unset).
GENERATE_DOCS=true

# Optional: manage dataset IAM during bootstrap
# Set to false if dataset IAM is centrally managed and you want to skip
# bindings like granting CI viewer on prod and PROD SA editor on prod.
MANAGE_DATASET_IAM=true

# Optional: Google Group for developers to receive project-level BigQuery Job User
# Example: DEV_GROUP_EMAIL=data-devs@your-domain.com
DEV_GROUP_EMAIL=

# BigQuery runtime tuning
# Maximum bytes billed per query (as integer bytes, e.g., 5000000000000 for ~5TB). Leave empty for no cap.
DBT_MAX_BYTES_BILLED=
# BigQuery priority: interactive (default) or batch
DBT_BQ_PRIORITY=interactive
# BigQuery job execution timeout in seconds (leave empty to disable)
DBT_JOB_EXECUTION_TIMEOUT_SEC=

# Optional: extra project-level roles to grant to dbt service accounts
# Many non-BigQuery roles are resource-scoped and not valid at the project level.
# Leave disabled unless you know the roles apply at project scope.
# Example: EXTRA_PROJECT_ROLES="roles/logging.logWriter roles/monitoring.metricWriter"
ENABLE_EXTRA_PROJECT_ROLES=false
EXTRA_PROJECT_ROLES=

# Developer specific runtime variables
DBT_TARGET=dev
DBT_GCP_PROJECT_DEV=${PROJECT_ID}
DBT_BQ_LOCATION=${BQ_LOCATION}
DBT_BQ_DATASET=${PROD_DATASET:-analytics}_${DBT_USER}   # per-dev dataset