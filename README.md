# dbt-core-gcloud-template

A starter template for **dbt Core on BigQuery** with:

* Per‚Äëdeveloper isolated datasets (dev), **integration (optional INT_DATASET)**, and **production**
* **GitHub Actions** for CI (Slim CI + `--defer`) and containerized CD
* **Cloud Run Jobs** orchestrated by **Cloud Scheduler** for prod
* Optional **Python pre/post hooks** and **dbt docs** artifacts (static)

> Why these choices:
> ‚Ä¢ Per‚Äëdeveloper sandboxes and environment separation are well‚Äëestablished best practices to reduce blast radius during development. Data teams often use developer‚Äëspecific schemas/datasets for isolation. ([Datafold][1])
> ‚Ä¢ "Slim CI" with `state:modified+` and `--defer` is a widely used pattern to speed up PR checks and validate only the impacted parts of the DAG. ([Medium][2], [Klaviyo Engineering][3])
> ‚Ä¢ Cloud Run **Jobs** + Cloud Scheduler provide serverless batch orchestration without maintaining servers; Jobs are designed for run‚Äëto‚Äëcompletion tasks and can be triggered on a schedule.

## üöÄ Choose Your Path

**New to this template?** Start with [Template Setup](#template-setup-new-deployment) to deploy the infrastructure and CI/CD pipeline.

**Developer joining an existing project?** Jump to [Developer Quick Start](#developer-quick-start) to get your local environment running.

**Need detailed reference?** See the [Complete Documentation](#complete-documentation) below.

---

## üìã Table of Contents

* [Template Setup (New Deployment)](#template-setup-new-deployment)
* [Developer Quick Start](#developer-quick-start)
* [Dev Container Setup](#dev-container-setup)
* [Daily Development Workflow](#daily-development-workflow)
* [Troubleshooting](#troubleshooting)
* [Complete Documentation](#complete-documentation)

---

## Template Setup (New Deployment)

*Use this section if you're setting up this template for the first time.*

### Prerequisites Check

Before starting, verify you have these tools installed:

```bash
# Check versions
python --version    # Should be 3.9-3.12
pip --version
gcloud version      # Google Cloud SDK
docker --version    # For building images
jq --version        # JSON processing
gh --version        # GitHub CLI (optional)
```

**GCP Requirements:**

* GCP project with billing enabled
* Owner permissions (temporarily for setup)
* APIs will be enabled automatically by the bootstrap script

### Quick Setup (4 Steps)

#### Step 1: Configure Environment

```bash
# Copy and edit the configuration
cp infra/.env.example infra/.env
$EDITOR infra/.env   # Set PROJECT_ID, REGION, GITHUB_REPO, etc.
```

**üí° Essential variables to set:**

* `PROJECT_ID` - Your GCP project ID
* `PROJECT_NUMBER` - Your GCP project number
* `REGION` - GCP region (e.g., us-central1)
* `GITHUB_REPO` - Your repo (e.g., myorg/my-dbt-project)
* `DBT_DOCS_BUCKET` - Globally unique bucket name

#### Step 2: Bootstrap GCP Infrastructure

```bash
(cd infra && ./10-bootstrap.sh)
```

**‚úÖ Success criteria:** Script completes without errors, shows "Bootstrap complete."

#### Step 3: Configure GitHub Integration

```bash
(cd infra && ./20-wif-github.sh)      # Set up OIDC
(cd infra && ./25-set-github-secrets.sh)  # Auto-configure repo secrets
```

**‚úÖ Success criteria:** GitHub repo secrets are set (check in GitHub Settings > Secrets)

#### Step 4: Deploy Production Job

```bash
(cd infra && ./30-deploy-cloud-run-job.sh)
(cd infra && ./40-schedule-prod-job.sh)
```

**‚úÖ Success criteria:** Cloud Run job exists and is scheduled

### Verify Your Setup

1. **Check GitHub Actions:** Push to main branch - CI/CD should trigger
2. **Check Cloud Run Job:** Should appear in GCP Console > Cloud Run > Jobs
3. **Check Scheduler:** Should appear in GCP Console > Cloud Scheduler

**üéâ Setup Complete!** Your template is now deployed. Developers can now use the [Developer Quick Start](#developer-quick-start).

---

## Developer Quick Start

*Use this section if you're joining an existing project that's already deployed.*

**Choose your development environment:**

* **Local Development**: Continue with the steps below
* **Dev Container**: See [Dev Container Setup](#dev-container-setup) for containerized development

### Prerequisites Check

Verify your local environment:

```bash
python --version    # 3.9-3.12
pip --version
gcloud version      # Google Cloud SDK
```

### Local Development (8 Steps)

#### Step 1: Set Up Python Environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

#### Step 2: Configure Environment

```bash
cp infra/.env.example infra/.env
$EDITOR infra/.env    # Ask your team lead for the correct values
```

**üí° Key variables for developers:**

* `DBT_GCP_PROJECT_DEV` - Development project
* `DBT_BQ_LOCATION` - BigQuery location
* Your `DBT_USER` will be auto-generated from your gcloud authenticated email

#### Step 3: Authenticate with GCP

```bash
gcloud auth login
gcloud auth application-default login
```

#### Step 4: Load Environment

```bash
# Load environment variables and generate DBT_USER
source ./setup-env.sh
```

#### Step 5: Set Up Your Development Dataset

```bash
./infra/50-ensure-dev-datasets.sh
```

**‚úÖ Success criteria:** Your personal dataset is created (e.g., `analytics_yourname`)

#### Step 6: Install dbt Dependencies

```bash
# Install packages defined in packages.yml
dbt deps
```

**‚úÖ Success criteria:** Dependencies installed successfully (e.g., GA4 dbt package)

#### Step 7: Verify dbt Setup

```bash
# Test dbt connection
dbt debug
```

**‚úÖ Success criteria:** `dbt debug` shows all connections OK

### VS Code Setup (Optional)

1. **Command Palette** ‚Üí "Python: Select Interpreter" ‚Üí Choose `.venv`
2. **Run/Debug:** Use built-in configurations for `dbt: debug`, `dbt: run`, `dbt: test`

**Note:** Remember to run `source ./setup-env.sh` in your terminal before using dbt commands directly in the terminal.

**üéâ Development Ready!** See [Daily Development Workflow](#daily-development-workflow) for your next steps.

---

## Dev Container Setup

*Use this section if you prefer containerized development with VS Code.*

### Prerequisites Check

Ensure you have:

```bash
# Required tools
docker --version        # Docker Desktop
```

**VS Code Extensions:**

* Dev Containers extension (`ms-vscode-remote.remote-containers`)

### Container Setup (3 Steps)

#### Step 1: Open in Container

```bash
# Clone and open in VS Code
git clone <your-repo-url>
cd <your-repo-name>
code .
```

When VS Code opens, click "**Reopen in Container**" when prompted, or use:

* `Ctrl+Shift+P` ‚Üí "Dev Containers: Reopen in Container"

#### Step 2: Configure Environment

The dev container automatically:

* Installs dbt-core and dbt-bigquery from `requirements.txt`
* Sets `DBT_PROFILES_DIR=profiles`
* Installs pre-commit hooks
* **Generates `DBT_USER`** from your gcloud authenticated email (for dataset isolation)
* **Loads `infra/.env`** variables into terminal sessions

You still need to configure your project variables:

```bash
cp infra/.env.example infra/.env
# Edit infra/.env with your project values
```

**Environment Notes:**

* The container runs a setup script that provides the same environment setup as local development
* Your `DBT_USER` will be automatically generated from your gcloud email and sanitized for BigQuery dataset naming
* All `infra/.env` variables are available in both terminal and VS Code debug sessions

#### Step 3: Authenticate with Google Cloud

```bash
# In the container terminal
gcloud auth login
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID
```

### Container Development

The dev container provides:

* **Debugging**: Use F5 or the Debug panel to run `dbt run`, `dbt test`, `dbt debug`
* **Integrated Terminal**: All dbt commands work directly
* **Extensions**: Python and dbt Power User extensions pre-installed

**Testing Your Setup:**

```bash
# Install dependencies first
dbt deps

# Test connection and run
dbt debug        # Should show all green checkmarks
dbt run --limit 1

# Verify environment is loaded
echo $DBT_USER   # Should show your sanitized username
echo $PROJECT_ID # Should show your GCP project
```

**Troubleshooting:**

* If `DBT_USER` is empty, restart the container to trigger the setup script
* If `infra/.env` variables aren't loaded, check that the file exists and is readable
* Environment setup logs appear in the container startup output

**üéâ Container Ready!** See [Daily Development Workflow](#daily-development-workflow) for your next steps.

---

## Daily Development Workflow

### Making Changes

```bash
# 1. Install/update dependencies (if packages.yml changed)
dbt deps

# 2. Create feature branch
git checkout -b feature/my-new-model

# 3. Make changes to models/
# Edit your .sql files...

# 4. Test changes locally
dbt build --select state:modified+

# 5. Compare dev vs prod data (optional)
dbt run-operation dev_prod_diff --args '{"table_name":"my_model", "limit": 100}'

# 6. Push and create PR
git push origin feature/my-new-model
gh pr create
```

### Common Commands

```bash
# Build only changed models and their dependencies
dbt build --select state:modified+

# Test everything
dbt test

# Generate docs locally
dbt docs generate --static
# Open target/index.html in browser

# Check data freshness
dbt source freshness
```

### Working with Data

* **Your data:** `analytics_yourname` dataset
* **Production data:** `analytics` dataset (read-only)
* **Compare environments:** Use the `dev_prod_diff` macro

---

## Troubleshooting

### Template Setup Issues

#### "Permission denied" during bootstrap

**Solution:** Ensure you have Project Owner role temporarily

```bash
# Check your permissions
(cd infra && ./check-operator-iam.sh)
```

#### "Bucket name already taken"

**Solution:** Change `DBT_DOCS_BUCKET` to something globally unique

```bash
# Edit infra/.env
DBT_DOCS_BUCKET=my-company-dbt-docs-$(date +%s)
```

#### GitHub secrets not set

**Solution:** Run the secrets script manually

```bash
(cd infra && ./25-set-github-secrets.sh)
```

### Developer Issues

#### `dbt debug` fails

**Solutions:**

1. Check authentication: `gcloud auth application-default login`
2. Verify project access: `gcloud projects describe YOUR_PROJECT_ID`
3. Ensure dataset exists: `(cd infra && ./50-ensure-dev-datasets.sh)`

#### "Dataset not found" error

**Solution:** Create your dev dataset

```bash
(cd infra && ./50-ensure-dev-datasets.sh)
```

#### VS Code not recognizing Python environment

**Solution:** Select the correct interpreter

1. **Cmd/Ctrl + Shift + P** ‚Üí "Python: Select Interpreter"
2. Choose `.venv/bin/python`

---

## Complete Documentation

*The sections below provide comprehensive reference information.*

## Prerequisites

* Python 3.9‚Äì3.12 with pip (or pipx). Verify: `python --version`, `pip --version`.
* Google Cloud SDK installed (provides `gcloud`, `bq`, and `gcloud storage`).
* Docker (to build/push the dbt image).
* jq (JSON CLI used by infra scripts).
* GitHub CLI (optional; required if you want to auto-set GitHub Actions secrets via `infra/25-set-github-secrets.sh`).
* GCP access to create IAM, BigQuery datasets, Artifact Registry repos, Cloud Run Jobs, and Cloud Scheduler jobs.
* APIs: BigQuery, Artifact Registry, Cloud Run, Cloud Scheduler, IAM (enabled by `infra/10-bootstrap.sh` if not already).

## Script Index & Typical Order

Core flow (typical first run):

* [infra/10-bootstrap.sh](#infra-bootstrap-one-time-per-gcp-project) ‚Äî Enable APIs; create SAs, Artifact Registry, datasets, docs bucket; set IAM.
* [infra/20-wif-github.sh](#infra-bootstrap-one-time-per-gcp-project) ‚Äî Configure GitHub OIDC / Workload Identity Federation.
* [infra/25-set-github-secrets.sh](#infra-bootstrap-one-time-per-gcp-project) ‚Äî Optional: set GitHub Actions repo secrets via gh.
* [infra/30-deploy-cloud-run-job.sh](#release-main-prod-container) ‚Äî Create/update the Cloud Run Job (prod runner).
* [infra/40-schedule-prod-job.sh](#release-main-prod-container) ‚Äî Create/update Cloud Scheduler trigger for the job.

Optional helpers (as needed):

* [infra/50-ensure-dev-datasets.sh](#developer-onboarding) ‚Äî Ensure per‚Äëdeveloper datasets + IAM (reads infra/devs.txt; supports --grant-job-user).
* [infra/70-configure-docs-website.sh](#serve-dbt-docs-http) ‚Äî Configure docs bucket website index; set PUBLIC=true for public read.
* [infra/80-deploy-docs-viewer.sh](#docs-viewer-cloud-run-iam-only) ‚Äî Deploy private Cloud Run docs viewer service (reads index.html from GCS).
* [infra/81-grant-docs-viewer-access.sh](#docs-viewer-cloud-run-iam-only) ‚Äî Grant run.invoker to users/groups for the docs viewer.
* [infra/85-ensure-notification-channels.sh](#monitoring) ‚Äî Create/reuse Monitoring email channels; writes NOTIFICATION_CHANNELS.
* [infra/90-monitoring.sh](#monitoring) ‚Äî Ensure alerting policies for Run Job, Scheduler, and BigQuery errors.
* [infra/15-artifacts-lifecycle.sh](#docs--artifacts) ‚Äî Set GCS lifecycle on artifacts/docs bucket (e.g., delete objects >30 days).
* [infra/check-operator-iam.sh](#operator-iam-to-run-infra-scripts) ‚Äî Check current operator‚Äôs project roles; helpful before running bootstrap.

Tip: 25/50/70/80/81/85/90/15 are optional and can be run any time after 10 (and when their prerequisites exist).

---

## Repo Layout

* `.github/workflows/ci.yml` ‚Äî PR validation on ephemeral BigQuery dataset (Slim CI, deferral) ([Medium][2], [Klaviyo Engineering][3])
* `.github/workflows/release.yml` ‚Äî Build & publish container ‚Üí update Cloud Run Job ‚Üí keep schedule in sync
* `Dockerfile`, `entrypoint.sh` ‚Äî Containerized dbt runner (supports optional hooks & docs)
* `profiles/profiles.yml` ‚Äî Env‚Äëvar driven dbt profiles for dev/ci/prod
* `models/` ‚Äî Example models/tests (staging ‚Üí marts)
* `scripts/` ‚Äî CI helpers (create/drop dataset, Slim CI, compare template)
* `infra/` ‚Äî One‚Äëtime GCP bootstrap (APIs, SAs, WIF/OIDC, datasets, Artifact Registry, Cloud Run Job, Scheduler)

  * Uses **Workload Identity Federation** for GitHub Actions (no long‚Äëlived keys).

### Local Development (Detailed Reference)

The sections above provide quick-start guides. For detailed reference:

**Environment Setup:**

* Configure variables in `infra/.env` (copied from `infra/.env.example`)
* `setup-env.sh` loads `infra/.env`, sets `DBT_PROFILES_DIR=profiles`, and generates `DBT_USER` from your gcloud email
* Ensure the profile name in `profiles/profiles.yml` matches `dbt_project.yml` (profile: `domainmethods`)

**Environment Variables:**

```bash
# Required
DBT_TARGET=dev
DBT_GCP_PROJECT_DEV=<your-dev-project>
DBT_BQ_LOCATION=US

# Optional overrides
# DBT_BQ_DATASET=analytics_${DBT_USER}
# DBT_MAX_BYTES_BILLED=
# DBT_JOB_EXECUTION_TIMEOUT_SEC=
```

> Tip: Use a per‚Äëdeveloper dataset like `${PROD_DATASET}_${DBT_USER}` so multiple engineers never collide. Per‚Äëdeveloper schemas/datasets are a common pattern. ([Datafold][1])

**Documentation Generation:**

```bash
dbt docs generate --static
# then open target/index.html
```

> The `--static` flag produces a single HTML doc you can host or share easily (no extra assets). See community write‚Äëups on hosting dbt docs as a static site. ([Hiflylabs][4], [Metaplane][5])

---

## Compare **Dev ‚Üî Prod**

Option A ‚Äî ad‚Äëhoc SQL in BigQuery UI:

* Use `scripts/compare_template.sql` (set `dev_project`, `dev_dataset`, `prod_project`, `prod_dataset`, and `table_name`).

Option B ‚Äî parameterized macro or helper:

* Export `DBT_USER` so the chosen naming (`${PROD_DATASET}_${DBT_USER}`) is stable in dev.
* Run one of:

```
dbt run-operation dev_prod_diff --args '{"table_name":"fct_example", "limit": 100}'
dbt run-operation dev_prod_diff_for_model --args '{"model_name":"fct_example", "limit": 100}'
scripts/compare.sh fct_example [dev_project] [prod_project]
```

* Override defaults with args: `dev_project`, `prod_project`, `dev_dataset_prefix`, `prod_dataset`, `execute_query`.

CI PR summary:

* The PR workflow runs diffs on changed models and publishes:
  * Per‚Äëmodel text files under the ‚Äúdata‚Äëdiff‚Äù artifact
  * A PR comment titled ‚ÄúDBT Data Diff Summary‚Äù with a table of row counts and diff counts
  * Set `DIFF_LIMIT` in CI to control how many differing rows are sampled in logs (default 200 in this template)

For deeper diffs, consider a **data‚Äëdiff** approach (row‚Äëlevel compare) to verify parity between environments. ([Datafold][6])

### Schema Diff in CI

In addition to row/data diffs, the CI pipeline can generate a schema comparison between the PR‚Äôs ephemeral dataset and production for changed models:

* Produces per‚Äëmodel reports under `schema_diff_reports/` with:
  * Column changes (added/removed/changed type/nullability)
  * Table type changes (TABLE ‚Üî VIEW)
  * Partitioning/clustering option changes
  * Movement detection (dataset/identifier changes via manifests)
* Writes a PR comment titled ‚Äúdbt Schema Diff Summary‚Äù
* Generates `schema_diff_reports/orphans.md` listing prod relations not covered by dbt models/sources
* Controlled by env: `DBT_GCP_PROJECT_CI`, `DBT_BQ_DATASET`, `DBT_GCP_PROJECT_PROD`, `DBT_BQ_DATASET_PROD`, optional `SCHEMA_DIFF_PROD_DATASETS`

---

## CI (Pull Requests with GitHub Actions)

**Secrets required**

* `GCP_WIF_PROVIDER` ‚Äî Workload Identity Federation provider resource
* `GCP_CI_SA_EMAIL` ‚Äî CI runner service account email
* `GCP_PROJECT_CI` ‚Äî GCP project to host ephemeral CI datasets
* `DBT_ARTIFACTS_BUCKET` ‚Äî GCS bucket for manifests/docs

**Flow**

1. OIDC auth in GitHub Actions (no JSON keys). Ensure `permissions: id-token: write`.
2. Create ephemeral BigQuery dataset `ci_pr_<PR>_<runid>` (cleaned up at end). The bootstrap grants the CI service account `roles/bigquery.user` so it can create datasets. The workflow then hardens ACLs so only the CI service account is `WRITER` on the dataset (removes `projectWriters`); everyone else is read‚Äëonly.
3. **Slim CI**: `dbt build --select state:modified+ --defer --state <prod_artifacts>` so only changed nodes + dependents run, resolving upstream refs to prod objects. ([Medium][2], [Klaviyo Engineering][3])
4. Generate docs and upload to `gs://$DBT_ARTIFACTS_BUCKET/ci/...`.
5. Always drop the ephemeral dataset.

> These Slim CI patterns (state selector + deferral) are documented in community engineering posts and case studies. ([Klaviyo Engineering][3], [Medium][2])

Note on INT_DATASET:

* INT_DATASET is optional. If set, the bootstrap script will create a standing non‚Äëprod dataset and grant the CI SA editor on it. If left empty, no integration dataset is created or bound; CI continues to use ephemeral per‚Äërun datasets only. If your CI runs in a different GCP project than the one you bootstrapped, ensure `roles/bigquery.user` is granted to the CI SA in that CI project.

---

## Release (main ‚Üí prod container)

**Secrets required**

* `GCP_WIF_PROVIDER`
* `GCP_PROD_SA_EMAIL` ‚Äî prod runner SA
* `GCP_PROJECT_PROD` ‚Äî prod project id
* `GCP_SCHEDULER_INVOKER_SA` ‚Äî Scheduler invoker SA email

**Flow**

1. Build Docker image and push to **Artifact Registry** (requires `roles/artifactregistry.writer` on the repo).
2. Create/update **Cloud Run Job** (e.g., `dbt-prod`) with that image.
3. Create/update **Cloud Scheduler** trigger on the job (‚ÄúTriggers‚Äù tab in the Job UI), or via CLI, to run on a cron schedule.
4. (Optional) Execute once after deploy to refresh prod artifacts.

Runtime defaults (Cloud Run Job):

* Runs `dbt build` then `dbt source freshness` by default in prod (set `RUN_FRESHNESS=false` to disable, or scope with `FRESHNESS_SELECT`).
* Uploads `manifest.json`, `run_results.json`, and `sources.json` to `gs://$DBT_ARTIFACTS_BUCKET/prod/`.
* Optionally generates docs (`GENERATE_DOCS=true`) and uploads `target/index.html` to `gs://$DBT_DOCS_BUCKET/index.html` (falls back to `DBT_ARTIFACTS_BUCKET` if `DBT_DOCS_BUCKET` is unset).

> Cloud Run **Jobs** are built for batch/ETL: they run to completion and can be executed on a schedule by Cloud Scheduler without standing up servers.

---

## Infra Bootstrap (one‚Äëtime per GCP project)

**Prereqs**: `gcloud`, `bq`, `jq`; logged in with `gcloud auth login` and billing enabled.

1. Copy and edit env

```bash
cp infra/.env.example infra/.env
$EDITOR infra/.env
```

2. Enable APIs, create SAs, **Artifact Registry** repo, datasets, docs bucket, and IAM:

```bash
(cd infra && ./10-bootstrap.sh)
```

* Artifact Registry repo creation uses `gcloud artifacts repositories create ...`.

Key env toggles (`infra/.env`):

* `INT_DATASET` (optional): create a standing non‚Äëprod dataset; leave blank for ephemeral‚Äëonly CI.
* `MANAGE_DATASET_IAM` (default true): manage dataset grants; set false if dataset IAM/ACLs are centrally managed.
* `PROD_HARDEN_ACL` (default true): ensure only the prod runner SA can write to the prod dataset; everyone else is query‚Äëonly.
* `INT_HARDEN_ACL` (default true): ensure only the CI SA can write to the standing integration dataset.

3. Configure GitHub **OIDC / Workload Identity Federation** (no keys):

```bash
(cd infra && ./20-wif-github.sh)
```

* Put the printed `workload_identity_provider` and `service_account` into GitHub Secrets (`GCP_WIF_PROVIDER`, `GCP_CI_SA_EMAIL`).
* We rely on the official `google-github-actions/auth` Action; OIDC requires `permissions: id-token: write` in the workflow.

3a. Set GitHub Actions secrets automatically (optional helper):

```
(cd infra && ./25-set-github-secrets.sh)           # uses GITHUB_REPO from infra/.env
# or pass repo explicitly
(cd infra && ./25-set-github-secrets.sh your-org/your-repo)
```

This sets:

* `GCP_WIF_PROVIDER`, `GCP_CI_SA_EMAIL`, `GCP_PROJECT_CI`, `DBT_ARTIFACTS_BUCKET`, `DBT_DOCS_BUCKET`
* `GCP_PROD_SA_EMAIL`, `GCP_PROJECT_PROD`, `GCP_SCHEDULER_INVOKER_SA`

Notes:

* The script uses `gh secret set` and values from `infra/.env`. It derives `GCP_WIF_PROVIDER` using the same pool/provider IDs as `20-wif-github.sh` (default `github-pool`/`github-provider`).
* `DBT_ARTIFACTS_BUCKET` defaults to `DBT_DOCS_BUCKET` unless `DBT_ARTIFACTS_BUCKET` is already exported in your shell.

4. After the first image exists (from the release workflow) you can deploy/update the job & Scheduler locally if needed:

```bash
(cd infra && ./30-deploy-cloud-run-job.sh)
(cd infra && ./40-schedule-prod-job.sh)
```

* Scheduling Cloud Run **Jobs** can be done directly from the Job‚Äôs **Triggers** tab or with API/CLI; the console path is explicit in Google‚Äôs docs.

5. Notification channels (optional helper for alerts):

```bash
(cd infra && ./85-ensure-notification-channels.sh)          # creates/reuses channels and updates .env + writes helper env file
# specify emails explicitly (otherwise uses DEV_GROUP_EMAIL or your active gcloud account)
(cd infra && ./85-ensure-notification-channels.sh --email [email¬†protected],[email¬†protected])
# export into current shell: source the script (it also writes infra/.notification_channels.env)
source infra/85-ensure-notification-channels.sh
```

6. Create a developer dataset (optional helper):

```bash
(cd infra && ./50-ensure-dev-datasets.sh)                 # operator + optional infra/devs.txt
(cd scripts/utils && ./create-dev-dataset.sh user@example.com dbt_alice_dev)   # one-off
```

* BigQuery IAM tip: **BigQuery Job User** at project level to run jobs, plus dataset‚Äëlevel roles to read/write the target dataset (least privilege).

---

## IAM & Security (minimal, least‚Äëprivilege)

* **CI SA**:

  * Project: `roles/bigquery.jobUser` to run jobs.
  * Dataset (CI dataset(s)): writer/editor as needed.
* **Prod SA** (Cloud Run Job runtime):

  * Project: `roles/bigquery.jobUser`
  * Prod datasets: writer/editor as needed.
* **Artifact Registry**:

  * GitHub build/publish needs `roles/artifactregistry.writer` on the target repo; runners log in via OIDC.
* **Scheduler ‚Üí Cloud Run Job**:

  * Give the Scheduler‚Äôs SA permission to invoke the job trigger (use the UI ‚ÄúAdd Scheduler Trigger‚Äù flow to wire this securely).

---

## Developer Onboarding

Ensure developers have a per‚Äëdev dataset and the minimal IAM needed to work safely.

1. Optional: grant project‚Äëlevel BigQuery Job User to a dev group once

   * Set `DEV_GROUP_EMAIL` in `infra/.env` (e.g., `data-devs@your-domain.com`).
   * Run `./infra/10-bootstrap.sh` (idempotent). This grants `roles/bigquery.jobUser` to the group.

   Alternatively, grant per-user Job User during dataset creation with `--grant-job-user`.

2. Add developers in `infra/devs.txt`

   * One entry per line: `email[,dataset]`. The dataset is optional.
   * If omitted, the script defaults to `${PROD_DATASET}_${short}`, where `short` is the email local-part lowercased and sanitized to `[a-z0-9_]`.

   Example `infra/devs.txt`:

   ```
   alice@example.com,analytics_alice
   bob@example.com
   # carol defaults to ${PROD_DATASET}_carol
   carol@example.com
   ```

3. Ensure datasets

   ```bash
   (cd infra && ./50-ensure-dev-datasets.sh)
   # or, to also grant project-level Job User per-user
   (cd infra && ./50-ensure-dev-datasets.sh --grant-job-user)
   ```

4. One-off: create a single dev dataset

   ```bash
   (cd scripts/utils && ./create-dev-dataset.sh dev@example.com [dataset] [--grant-job-user])
   ```

Notes:

* Datasets and IAM settings are idempotent; you can re-run safely after editing `infra/devs.txt`.
* Group-based Job User is recommended at scale. Use `--grant-job-user` for exceptions.

---

## Operator IAM (to run infra scripts)

The person/service running scripts in `infra/` needs elevated, temporary permissions to create resources and set IAM. Easiest is Project Owner during bootstrap; for least‚Äëprivilege, grant these roles to the operator and remove afterward:

* Project-wide roles:
  * `roles/serviceusage.serviceUsageAdmin` (enable required APIs)
  * `roles/iam.serviceAccountAdmin` (create service accounts)
  * `roles/iam.serviceAccountIamAdmin` (set IAM policy on service accounts)
  * `roles/iam.workloadIdentityPoolAdmin` (create/update WIF pool/provider)
  * `roles/artifactregistry.admin` (create Artifact Registry repo)
  * `roles/storage.admin` (create bucket + set IAM)
  * `roles/bigquery.admin` (create datasets + set dataset IAM)
  * `roles/run.admin` (create/update Cloud Run Jobs + set IAM)
  * `roles/cloudscheduler.admin` (create/update Scheduler jobs)
* On the specific service accounts created by bootstrap:
  * `roles/iam.serviceAccountUser` on the prod runner SA (required to deploy a Run Job with `--service-account`)
  * `roles/iam.serviceAccountUser` on the scheduler invoker SA (if you need to impersonate it during setup)

Notes:

* These are for the operator only. Runtime SAs (CI and Prod) use the minimal roles listed above in ‚ÄúIAM & Security‚Äù.
* Many orgs grant Project Owner to the bootstrapper, run `infra/10-bootstrap.sh` and `infra/20-wif-github.sh`, then revoke and keep least‚Äëprivilege going forward.
* Important: Project Owner does not implicitly include BigQuery dataset admin. To change dataset IAM (e.g., grant the CI/Prod SAs reader/editor on `analytics`), the operator needs `roles/bigquery.admin` (or dataset‚Äëlevel ownership). If missing, `infra/10-bootstrap.sh` will warn with `bigquery.datasets.setIamPolicy` and continue.
* You can disable dataset IAM management entirely by setting `MANAGE_DATASET_IAM=false` in `infra/.env`. The bootstrap will then skip those bindings and avoid warnings in locked‚Äëdown environments.

Check your current permissions:

```
(cd infra && bash ./check-operator-iam.sh)
```

The script compares your active gcloud principal against the required project roles and flags any missing ones. It also suggests `serviceAccountUser` bindings on specific SAs if needed. It checks direct bindings; if you receive roles via a group, it may not detect that.

### Troubleshooting: dataset IAM warnings in bootstrap

If you see lines like:

```
[warn] Failed to set IAM on dataset dm-website-426721:analytics. Check your permissions (need bigquery.datasets.setIamPolicy). Continuing.
```

This means the operator does not have permission to modify dataset IAM. Options:

* Grant the operator `roles/bigquery.admin` temporarily and re‚Äërun `infra/10-bootstrap.sh`.
* Have a privileged admin grant the CI SA `roles/bigquery.dataViewer` on prod datasets and the Prod SA `roles/bigquery.dataEditor`.
* Ignore the warnings if dataset IAM is managed centrally; the rest of bootstrap (APIs, SAs, bucket IAM, etc.) proceeds.

---

## Docs & Artifacts

* CI publishes `manifest.json` and optional static docs to your `DBT_ARTIFACTS_BUCKET`.
* For static docs, we use `dbt docs generate --static` and host a single HTML file if needed (see community examples of static hosting). ([Hiflylabs][4], [Metaplane][5])

---

## Serve dbt Docs (HTTP)

The job can generate a single-file site (`target/index.html`) via `dbt docs generate --static` and upload it to the docs bucket. You have a few options to view it over HTTP:

* Option A ‚Äî Simple public website (fastest)
  * Make the docs bucket a static website and grant public read.
  * Configure and print URL:
    * `(cd infra && PUBLIC=true ./70-configure-docs-website.sh)`
    * Then open: `https://storage.googleapis.com/${DBT_DOCS_BUCKET}/index.html`
  * Caution: Public means world-readable. Use only if acceptable.

* Option B ‚Äî Private access (no public read)
  * Keep the bucket private (default). View via Cloud Console (Storage > Buckets > your bucket > `index.html` > Open in browser) when authenticated.
  * For a private HTTP endpoint, consider a Cloud Run proxy or Load Balancer + CDN with IAM integration. Ask if you want me to add a tiny Cloud Run service that serves the file from GCS using the service account.

* Option C ‚Äî Load Balancer + CDN (prod-grade)
  * Create an HTTPS Load Balancer with a backend bucket pointing to your docs bucket, enable Cloud CDN, and add a managed certificate for a custom domain (e.g., `docs.example.com`).
  * This keeps the bucket private to Google and exposes content via the edge proxy; can be combined with Identity-Aware Proxy (IAP) for auth.

Note: The infra bootstrap script creates the docs bucket but does not make it public. Use `infra/70-configure-docs-website.sh` to enable static website and optional public access.

### Docs Viewer (Cloud Run, IAM‚Äëonly)

For private, simple access without IAP or a load balancer, deploy a small Cloud Run service that reads `index.html` from your docs bucket:

1) Ensure the Docs Viewer service account exists and has read access (created by `infra/10-bootstrap.sh` as `DOCS_VIEWER_SA_ID`).
2) Build and deploy the service:

```
(cd infra && ./80-deploy-docs-viewer.sh)
```

3) Grant users/groups access to view (run.invoker):

```
(cd infra && ./81-grant-docs-viewer-access.sh group:[email¬†protected])
```

4) Open the Cloud Run service URL printed by the deploy script.

Runtime envs:

* `DBT_DOCS_BUCKET` (required): bucket that contains `index.html`.
* `DOCS_INDEX_OBJECT` (optional): defaults to `index.html`.
* `DOCS_CACHE_CONTROL` (optional): default `public, max-age=60`.

Automatic upload on prod runs:

* If the prod job sets `GENERATE_DOCS=true` and `DBT_DOCS_BUCKET=<bucket>`, the dbt container uploads `target/index.html` to `gs://<bucket>/index.html` automatically after generation.

---

## Monitoring

Set up basic monitoring for failures across the stack using log‚Äëbased metrics and alerting policies.

* Metrics: `cloud_run_job_errors`, `cloud_run_job_failures` (text match), `cloud_scheduler_job_errors`, `bigquery_job_errors`.
* Policies: alert when count > 0 over 5 minutes; uses `gcloud beta/alpha monitoring policies`.
* Channels: optional but recommended via `infra/85-ensure-notification-channels.sh` (sets `NOTIFICATION_CHANNELS`).

Usage

* Create channels (optional):
  * `(cd infra && ./85-ensure-notification-channels.sh --email [email¬†protected],[email¬†protected])`
  * The script updates `infra/.env` and can export a helper file `.notification_channels.env`.
* Create metrics + policies:
  * `(cd infra && ./90-monitoring.sh)`
* Override names/regions (optional):
  * `JOB_NAME=dbt-prod-run SCHED_JOB_NAME=dbt-prod-nightly REGION=us-central1 SCHED_REGION=us-central1 (cd infra && ./90-monitoring.sh)`

Notes

* Requires an up‚Äëto‚Äëdate `gcloud` with Monitoring alpha/beta commands.
* Safe to re‚Äërun; existing metrics/policies are left in place.

## Operational Notes

* **BigQuery mapping**: In BigQuery, datasets are the logical containers for tables/views; this template uses dataset boundaries to isolate environments and developers.
* **Slim CI + deferral** keeps PR runs fast and realistic by resolving unchanged upstream refs to prod objects. Community case studies detail this approach and command flags. ([Medium][2], [Klaviyo Engineering][3])
* **Cloud Run Job timeouts**: Jobs are designed for long‚Äërunning work with configurable task timeouts (much longer than Cloud Run services‚Äô request timeouts).
* **Docker auth to Artifact Registry**: If you push locally, configure Docker credential helper: `gcloud auth configure-docker <region>-docker.pkg.dev`.

---

## Next Steps

* Replace placeholder IDs in workflows and `infra/.env`.
* Decide on the dataset naming convention. This template uses `${PROD_DATASET}_${DBT_USER}` for dev by default.
* Add models/tests and any packages to `packages.yml`.
* Consider a formal **data diff** step in CI to compare dev vs prod tables on changed models. ([Datafold][6])

---

## Acknowledgements (Inspiration & References)

* **Per‚Äëdeveloper environments & environment strategy** ‚Äî Datafold‚Äôs guide on dbt development environments. ([Datafold][1])
* **Slim CI with `state:modified+` & `--defer`** ‚Äî Implementation notes and examples from melbdataguy and Klaviyo Engineering. ([Medium][2], [Klaviyo Engineering][3])
* **Data Diff Concept** ‚Äî Overview of table‚Äëlevel diffing for dev vs prod validation. ([Datafold][6])

---

### References

[1]: https://www.datafold.com/blog/how-to-setup-dbt-development-environments "Optimizing dbt development environments | Datafold"
[2]: https://melbdataguy.medium.com/implementing-ci-cd-for-dbt-core-with-bigquery-and-github-actions-f930d48a674b "Implementing CI/CD for dbt-core with BigQuery and Github Actions | by melbdataguy | Medium"
[3]: https://klaviyo.tech/continuous-integration-with-dbt-part-2-47c093a0548e "Continuous Integration with dbt (Part 2) | by Corey Angers | Klaviyo Engineering"
[4]: https://hiflylabs.com/blog/2023/3/16/dbt-docs-as-a-static-website?utm_source=chatgpt.com "dbt Docs as a Static Website"
[5]: https://www.metaplane.dev/blog/host-and-share-dbt-docs?utm_source=chatgpt.com "3 ways to host and share dbt docs"
[6]: https://www.datafold.com/blog/what-the-heck-is-data-diffing?utm_source=chatgpt.com "What the heck is data diffing?!"
